{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the code from this kaggle kernel [wingbeats-basic-py](https://www.kaggle.com/potamitis/wingbeats-basic-py), which is governed by Apache 2.0 license, copyright of Ilyas Potamitis.\n",
    "\n",
    "The data comes from this kaggle dataset [Wingbeats](https://www.kaggle.com/potamitis/wingbeats/data).\n",
    "\n",
    "The data collection process is described in the following paper:\n",
    "\n",
    "[http://ieeexplore.ieee.org/abstract/document/7482663/](http://ieeexplore.ieee.org/abstract/document/7482663/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will import many packages that we will need\n",
    "\n",
    "from __future__ import division\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore some deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the stored data\n",
    "X = np.load('data/X.npy')\n",
    "y = np.load('data/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Ae. aegypti', 'Ae. albopictus', 'An. gambiae', 'An. arabiensis', 'C. pipiens', 'C. quinquefasciatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "fs = 100 # frequency\n",
    "XX = np.zeros((X.shape[0],129)).astype(\"float32\")   # allocate space\n",
    "for i in range(X.shape[0]):\n",
    "    XX[i] = 10*np.log10(signal.welch(X[i], fs=fs, window='hanning', nperseg=256, noverlap=128+64)[1])\n",
    "\n",
    "# Show one recording\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.linspace(0,X.shape[1]/fs,X.shape[1]),X[0])\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.xlabel('time (s)')\n",
    "plt.title('Wingbeat recording')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.linspace(0,fs/2,129),XX[0])\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [dB]')\n",
    "plt.title('Welch Power Spectral Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to later evaluate the performance of the classifiers we split the dataset into a training and testing part, where the testing part is 20% of the total dataset. We use the `train_test_split` function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.20, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use a [Support Vector Classifier](http://scikit-learn.org/stable/modules/svm.html#mathematical-formulation) with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A quick result\n",
    "model = SVC(kernel = 'linear', C=0.01, cache_size=200)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(\"Name: %s, ac: %f\" % ('SVC', ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:*** \n",
    "* Try the support vector classifier with an rbf kernel, gamma kernel. How does this affect the performance?\n",
    "* Modify the regularization parameter C. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Nonlinear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A quick result\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.20, random_state=2018)\n",
    "# fit model on training data\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(\"Name: %s, ac: %f\" % ('Gradient Boosting', ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "* try a [ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html), [Random Forest classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(default kernel classifier with rbf fails miserably)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *empirical accuracy* of the classifier is defined as \n",
    "$\\sum_{i=1}^N \\mathbb{1}\\{g(X_i)==Y_i\\}/N$, i.e. we are counting for how many observations we predicted the correct label. The accuracy is an overall measure and it does not tell us where the error happens. It is often useful to look at the `confusion matrix` which contains different types of errors for each category. The rows correspond to the true labels, the columns correspond to the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, columns=target_names, index=target_names)\n",
    "plt.figure(figsize = (15,10))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "df_cm = pd.DataFrame(cm, columns=target_names, index=target_names)\n",
    "plt.figure(figsize = (15,10))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in general we observe samples from the joint distribution $(X,Y)$. The *expected error* of a classifier $g$ is $P(g(X) \\ne Y)$.\n",
    "\n",
    "We would like to minimize the *generalization error*:\n",
    "\n",
    "      generalization error = expected error - empirical error \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Above we split our dataset in two parts - train and test. To alleviate the effect of evaluating the performance only on a single dataset, we perform cross-validation by subsetting the dataset in several different train-test combinations and then average the performance over the set of testing datasets we have created. We obtain also information about the variance of the error, which might be more important than the actual mean value (and can guide us to a more robust classifier - performing similarly on different subsets).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model contains the result of the gradient boosting classifier\n",
    "acc_folds = cross_val_score(model, XX, y, cv=5)\n",
    "print(\"Mean: %f, Std: %f\" % (np.mean(acc_folds), np.std(acc_folds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classifiers we want to explore\n",
    "names = [\"Gradient Boosting\", \"Random Forest\", \"ExtraTreesClassifier\", \"Linear SVM\", \"RBF SVM\"]\n",
    "\n",
    "classifiers = [\n",
    "\tGradientBoostingClassifier(n_estimators=650, learning_rate=0.2),\n",
    "    RandomForestClassifier(n_estimators = 650, min_samples_split = 3, min_samples_leaf = 2, random_state = 2018, n_jobs=-1),\n",
    "    ExtraTreesClassifier(n_estimators = 650, random_state = 2018, n_jobs=-1),\n",
    "    SVC(kernel=\"linear\", C=0.01),\n",
    "    SVC(gamma=0.008, C=.1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CV experiments - it will take long..\n",
    "# Species level\n",
    "for name, clf in zip(names, classifiers):\n",
    "\tprint(name)\n",
    "\tclf_ = make_pipeline(StandardScaler(), clf)\n",
    "\tacc_folds = cross_val_score(clf_, XX, y, cv=5)\n",
    "\tprint(\"Mean: %f, Std: %f\" % (np.mean(acc_folds), np.std(acc_folds)))\n",
    "\tprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the Gradient Boosting classifier is best. This is a generally quite robust nonlinear classifier and often outperforms Random Forests. The disadvantage is that it is more computationally expensive and harder to parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks:\n",
    "* the cross validation error is a slightly biased  \n",
    "* when doing parameter selection we can be prone to over-fitting even with cross-validation\n",
    "* when doing dimensionality reduction we should make sure we do it only on the training set withing the fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Unabalanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the binary classification problem. We will pick 'Ae. albopictus' as the 1-class, and all other ones will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert y to binary\n",
    "y_binary = (y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only about 11 percent of samples are True\n",
    "sum(y_binary)/len(y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A quick result\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y_binary, test_size=0.20, random_state=2018)\n",
    "# fit model no training data\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print(\"Name: %s, ac: %f\" % ('Gradient Boosting', ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is that?\n",
    "\n",
    "Let's compare to the constant predictor which always predicts zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a predictor which always predicts zero\n",
    "y_pred0 = np.zeros(len(y_pred))\n",
    "ac = accuracy_score(y_test, y_pred0)\n",
    "print(\"Name: %s, ac: %f\" % ('Zero Predictor', ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not bad either, but we will miss identifying any mosquitos of that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operator Characteristic (ROC) & Precision-Recall (PR) Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = model.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the precision recall curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curve\n",
    "plt.figure()\n",
    "roc_auc = auc(fpr, tpr)\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the precision recall curve\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks:\n",
    "* We can see the ROC curve is increasing\n",
    "* The P-R curve is much more irregular\n",
    "* There is though a 1-1 correspondence between the curves\n",
    "* When working with unbalanced dataset you should consider a stratified cross-validation\n",
    "* You can balance the dataset the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Exercise:*** \n",
    "calculate the roc/precision-recall curves for several classifiers, and decide which one is better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
